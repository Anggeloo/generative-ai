{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHTImBt37VsR"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv9og4qtpaf3"
      },
      "source": [
        "# Build your own Grounded RAG application using Vertex AI Search Standalone APIs for RAG and LangChain\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/search/retrieval-augmented-generation/vertex_ai_search_standalone_apis.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fsearch%2Fretrieval-augmented-generation%2Fvertex_ai_search_standalone_apis.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/search/retrieval-augmented-generation/vertex_ai_search_standalone_apis.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/search/retrieval-augmented-generation/vertex_ai_search_standalone_apis.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QMfdiiIptH6"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Abhishek Bhagwat](https://github.com/Abhishekbhagwat), [Rajesh Thallam](https://github.com/RajeshThallam), [Holt Skinner](https://github.com/holtskinner) |\n",
        "| Reviewers(s) | [Alan Blount](https://github.com/zeroasterisk), [Skander Hannachi](https://github.com/SkanderHn)|\n",
        "| Last updated | 2024-08-07 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vme9HBwvqK2u"
      },
      "source": [
        "# üìå Overview\n",
        "\n",
        "In this notebook, we show you how to use the [Vertex AI Search Component APIs for RAG](https://cloud.google.com/generative-ai-app-builder/docs/builder-apis) to build a custom search solution on your own documents.\n",
        "\n",
        "---\n",
        "\n",
        "Building a robust custom (DIY) Retrieval Augmented Generation (RAG) system for grounding can be challenging. Vertex AI Search simplifies the process with a suite of flexible standalone APIs to help you create your own search solutions.\n",
        "\n",
        "* **[Document AI Layout Parser](https://cloud.google.com/document-ai/docs/layout-parse-chunk)**: Transforms documents into structured representations, making content easily accessible. Creates context-aware chunks for improved information retrieval in generative AI and discovery applications.\n",
        "* **[Ranking API](https://cloud.google.com/generative-ai-app-builder/docs/ranking)**: Re-ranks search results based on relevance to the original query. Enhances RAG accuracy by optimizing retrieval beyond initial nearest neighbor search.\n",
        "* **[Check Grounding API](https://cloud.google.com/generative-ai-app-builder/docs/check-grounding)**: Acts as a \"validator\" to determine whether statements or claims are supported by provided facts (essentially how grounded a given piece of text is in a given set of reference text). Enables online flagging of ungrounded responses and offline evaluation of generative responses.\n",
        "* **[Grounded Generation API](https://cloud.google.com/generative-ai-app-builder/docs/grounded-gen)**: Generate grounded answers to prompts based on the input data.\n",
        "\n",
        "**Key Features**:\n",
        "\n",
        "* **Leverage Vertex AI Search technology**:  Build custom RAG and Grounded Generation solutions using the same technology that powers Vertex AI Search.\n",
        "* **Granular control**: Tailor your RAG system to specific use cases and offer greater control to your users.\n",
        "* **Seamless integration**: Combine these APIs with core services like Embeddings API and Vector Search for advanced grounded AI applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmEm2eG_YfOD"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8D9ua6KHPsw"
      },
      "source": [
        "# üìê Architecture\n",
        "\n",
        "Following is a high-level architecture of what we will build in this notebook.\n",
        "\n",
        "You will perform the following steps:\n",
        "\n",
        "- **Step 1. Data Ingestion:** Parse the documents in a Cloud Storage bucket using [Document AI Layout Parser](https://cloud.google.com/document-ai/docs/layout-parse-chunk) and convert the raw text chunks as embeddings using the [Vertex AI Embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings). The generated embeddings power semantic search using [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview) (vector database).\n",
        "\n",
        "- **Step 2. Retrieval:** Retrieve relevant chunks from Vertex AI Vector Search for a given user query and re-rank the chunks using [Ranking API](https://cloud.google.com/generative-ai-app-builder/docs/ranking).\n",
        "\n",
        "- **Step 3. Answer generation:** Use the [Grounded Generation API](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-multimodal-prompts) to generate an answer for the given user query based on the re-ranked chunks retrieved from the vector search.\n",
        "\n",
        "- **Step 4. Answer validation:** The generated answer is validated with [Check Grounding API](https://cloud.google.com/generative-ai-app-builder/docs/check-grounding) to determine how grounded the answer is to the relevant chunks retrieved.\n",
        "\n",
        "The notebook uses the [Google Cloud + LangChain integrations](https://python.langchain.com/v0.1/docs/integrations/platforms/google/) to orchestrate the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UurHrhD2Vswq"
      },
      "source": [
        "![vais-standalone-architecture.png](https://storage.googleapis.com/github-repo/search/vais-standalone-apis/vais-standalone-architecture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58YeBDniYhOB"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VwREY0Orpy_"
      },
      "source": [
        "# üé¨ Getting Started\n",
        "\n",
        "The following steps are necessary to run this notebook, no matter what notebook environment you're using.\n",
        "\n",
        "If you're entirely new to Google Cloud, [get started here](https://cloud.google.com/docs/get-started).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs2D1ccZL6me"
      },
      "source": [
        "### Google Cloud Project Setup\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "1. [Enable the Service Usage, Vertex AI, Cloud Storage, Document AI, Discovery Engine APIs](https://console.cloud.google.com/flows/enableapi?apiid=serviceusage.googleapis.com,aiplatform.googleapis.com,storage.googleapis.com,documentai.googleapis.com,discoveryengine.googleapis.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-PlSE6cMdnr"
      },
      "source": [
        "### Google Cloud Permissions\n",
        "\n",
        "**To run the complete Notebook, including the optional section, you will need to have the [Owner role](https://cloud.google.com/iam/docs/understanding-roles) for your project.**\n",
        "\n",
        "If you want to skip the optional section, you need at least the following [roles](https://cloud.google.com/iam/docs/granting-changing-revoking-access):\n",
        "* **`roles/serviceusage.serviceUsageAdmin`** to enable APIs\n",
        "* **`roles/iam.serviceAccountAdmin`** to modify service agent permissions\n",
        "* **`roles/aiplatform.user`** to use AI Platform components\n",
        "* **`roles/storage.objectAdmin`** to modify and delete GCS buckets\n",
        "* **`roles/documentai.admin`** to create and use Document AI Processors\n",
        "* **`roles/discoveryengine.admin`** to modify Vertex AI Search assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ2v8OJiL45C"
      },
      "source": [
        "### Install Vertex AI SDK and Other Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrT13CaUro6S"
      },
      "outputs": [],
      "source": [
        "%pip install google-cloud-aiplatform google-cloud-discoveryengine google-cloud-documentai google-cloud-documentai-toolbox google-cloud-storage langchain-google-vertexai langchain-google-community[vertexaisearch,docai] rich --upgrade --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFMd7eFqsNof"
      },
      "source": [
        "### Restart Runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_qr2n2SsJ81"
      },
      "outputs": [],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDhypUTlsPUZ"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>‚ö†Ô∏è The kernel is going to restart. Please wait until it is finished before continuing to the next step. ‚ö†Ô∏è</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEd6ie39sRcv"
      },
      "source": [
        "### Authenticate\n",
        "\n",
        "If you're using Colab, run the code in the next cell. Follow the popups and authenticate with an account that has access to your Google Cloud [project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects).\n",
        "\n",
        "If you're running this notebook somewhere besides Colab, make sure your environment has the right Google Cloud access. If that's a new concept to you, consider looking into [Application Default Credentials for your local environment](https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev) and [initializing the Google Cloud CLI](https://cloud.google.com/docs/authentication/gcloud). In many cases, running `gcloud auth application-default login` in a shell on the machine running the notebook kernel is sufficient.\n",
        "\n",
        "More authentication options are discussed [here](https://cloud.google.com/docs/authentication)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IjPUoABsTGY"
      },
      "outputs": [],
      "source": [
        "# Colab authentication.\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elli6J978x_E"
      },
      "source": [
        "### Set Google Cloud project information and Initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
        "\n",
        "Make sure to change `PROJECT_ID` in the next cell. You can leave the values for `REGION` unless you have a specific reason to change them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTpmZ97tutA7"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b42bd01a88a6"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8134c98aa3b1"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "from typing import List, Optional\n",
        "import uuid\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "from google.api_core.client_options import ClientOptions\n",
        "from google.cloud import aiplatform, discoveryengine, documentai, storage\n",
        "from google.cloud.aiplatform import MatchingEngineIndex, MatchingEngineIndexEndpoint\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_community.document_loaders.gcs_directory import GCSDirectoryLoader\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, chain\n",
        "from langchain_google_community import VertexAICheckGroundingWrapper, VertexAIRank\n",
        "from langchain_google_community.docai import DocAIParser\n",
        "from langchain_google_vertexai import VertexAI\n",
        "from langchain_google_vertexai.embeddings import VertexAIEmbeddings\n",
        "from langchain_google_vertexai.vectorstores.vectorstores import VectorSearchVectorStore\n",
        "import markdown as md\n",
        "from rich import print"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFN9SP7hlFWZ"
      },
      "source": [
        "### Initialize variables\n",
        "\n",
        "Set the values for the name of your project. \n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "‚ìò You might already have all of these resources created in which case you should use their names and set <code>CREATE_RESOURCES=False</code>. If you do not already have this all created, you should set new names for your Cloud Storage bucket, index, index endpoint, and Document AI processor.\n",
        "</div>\n",
        "\n",
        "**TIP:** stick to `hyphenated-lower-case` naming conventions, and use the same project name as a component of each of these names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SPE9etVVA70"
      },
      "outputs": [],
      "source": [
        "# Cloud storage buckets\n",
        "GCS_BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "GCS_OUTPUT_PATH = f\"{GCS_BUCKET_URI}\"  # DocAI Layout Parser Output Path\n",
        "GCS_BUCKET_NAME = GCS_BUCKET_URI.replace(\"gs://\", \"\")\n",
        "\n",
        "# Vertex AI Vector Search\n",
        "# parameter description here\n",
        "# https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.MatchingEngineIndex#google_cloud_aiplatform_MatchingEngineIndex_create_tree_ah_index\n",
        "VS_INDEX_NAME = \"[your-index-name]\"  # @param {type:\"string\"}\n",
        "VS_INDEX_ENDPOINT_NAME = \"[your-index-endpoint-name]\"  # @param {type:\"string\"}\n",
        "VS_CONTENTS_DELTA_URI = f\"{GCS_BUCKET_URI}/index/embeddings\"\n",
        "VS_DIMENSIONS = 768\n",
        "VS_APPROX_NEIGHBORS = 150\n",
        "VS_INDEX_UPDATE_METHOD = \"STREAM_UPDATE\"\n",
        "VS_INDEX_SHARD_SIZE = \"SHARD_SIZE_SMALL\"\n",
        "VS_LEAF_NODE_EMB_COUNT = 500\n",
        "VS_LEAF_SEARCH_PERCENT = 80\n",
        "VS_DISTANCE_MEASURE_TYPE = \"DOT_PRODUCT_DISTANCE\"\n",
        "VS_MACHINE_TYPE = \"e2-standard-16\"\n",
        "VS_MIN_REPLICAS = 1\n",
        "VS_MAX_REPLICAS = 1\n",
        "VS_DESCRIPTION = \"Index for DIY RAG with Vertex AI APIs\"  # @param {type:\"string\"}\n",
        "\n",
        "# Models\n",
        "EMBEDDINGS_MODEL_NAME = \"text-embedding-004\"\n",
        "LLM_MODEL_NAME = \"gemini-1.5-pro\"\n",
        "\n",
        "# DocumentAI Processor\n",
        "DOCAI_LOCATION = \"us\"  # @param [\"us\", \"eu\"]\n",
        "DOCAI_PROCESSOR_NAME = \"[your-docai-processor-name]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Enable/disable flags\n",
        "# flag to create Google Cloud resources configured above\n",
        "# refer to the notes before this cell\n",
        "CREATE_RESOURCES = False  # @param {type:\"boolean\"}\n",
        "# flag to run data ingestion\n",
        "RUN_INGESTION = True  # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zFmooN09e44F"
      },
      "outputs": [],
      "source": [
        "# @title Utility function to create resources\n",
        "\n",
        "\n",
        "def create_uuid(name: str) -> str:\n",
        "    hex_string = hashlib.md5(name.encode(\"UTF-8\")).hexdigest()\n",
        "    return str(uuid.UUID(hex=hex_string))\n",
        "\n",
        "\n",
        "def create_bucket(bucket_name: str) -> storage.Bucket:\n",
        "    # create Cloud Storage bucket if does not exists\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    if bucket.exists():\n",
        "        print(f\"Bucket {bucket.name} exists\")\n",
        "        return bucket\n",
        "\n",
        "    if not CREATE_RESOURCES:\n",
        "        return bucket\n",
        "\n",
        "    bucket = storage_client.create_bucket(bucket_name)\n",
        "    print(f\"Bucket {bucket.name} created\")\n",
        "    return bucket\n",
        "\n",
        "\n",
        "def create_index() -> Optional[MatchingEngineIndex]:\n",
        "    index_names = [\n",
        "        index.resource_name\n",
        "        for index in MatchingEngineIndex.list(filter=f\"display_name={VS_INDEX_NAME}\")\n",
        "    ]\n",
        "\n",
        "    if len(index_names) > 0:\n",
        "        vs_index = MatchingEngineIndex(index_name=index_names[0])\n",
        "        print(\n",
        "            f\"Vector Search index {vs_index.display_name} exists with resource name {vs_index.resource_name}\"\n",
        "        )\n",
        "        return vs_index\n",
        "\n",
        "    if not CREATE_RESOURCES:\n",
        "        print(\n",
        "            f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"\n",
        "        )\n",
        "        return None\n",
        "\n",
        "    print(f\"Creating Vector Search index {VS_INDEX_NAME} ...\")\n",
        "    vs_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
        "        display_name=VS_INDEX_NAME,\n",
        "        dimensions=VS_DIMENSIONS,\n",
        "        approximate_neighbors_count=VS_APPROX_NEIGHBORS,\n",
        "        distance_measure_type=VS_DISTANCE_MEASURE_TYPE,\n",
        "        leaf_node_embedding_count=VS_LEAF_NODE_EMB_COUNT,\n",
        "        leaf_nodes_to_search_percent=VS_LEAF_SEARCH_PERCENT,\n",
        "        description=VS_DESCRIPTION,\n",
        "        shard_size=VS_INDEX_SHARD_SIZE,\n",
        "        index_update_method=VS_INDEX_UPDATE_METHOD,\n",
        "        project=PROJECT_ID,\n",
        "        location=REGION,\n",
        "    )\n",
        "    print(\n",
        "        f\"Vector Search index {vs_index.display_name} created with resource name {vs_index.resource_name}\"\n",
        "    )\n",
        "    return vs_index\n",
        "\n",
        "\n",
        "def create_index_endpoint() -> Optional[MatchingEngineIndexEndpoint]:\n",
        "    endpoint_names = [\n",
        "        endpoint.resource_name\n",
        "        for endpoint in MatchingEngineIndexEndpoint.list(\n",
        "            filter=f\"display_name={VS_INDEX_ENDPOINT_NAME}\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    if len(endpoint_names) > 0:\n",
        "        vs_endpoint = MatchingEngineIndexEndpoint(index_endpoint_name=endpoint_names[0])\n",
        "        print(\n",
        "            f\"Vector Search index endpoint {vs_endpoint.display_name} exists with resource name {vs_endpoint.resource_name}\"\n",
        "        )\n",
        "        return vs_endpoint\n",
        "\n",
        "    if not CREATE_RESOURCES:\n",
        "        print(\n",
        "            f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"\n",
        "        )\n",
        "        return None\n",
        "\n",
        "    print(f\"Creating Vector Search index endpoint {VS_INDEX_ENDPOINT_NAME} ...\")\n",
        "    vs_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
        "        display_name=VS_INDEX_ENDPOINT_NAME,\n",
        "        public_endpoint_enabled=True,\n",
        "        description=VS_DESCRIPTION,\n",
        "        project=PROJECT_ID,\n",
        "        location=REGION,\n",
        "    )\n",
        "    print(\n",
        "        f\"Vector Search index endpoint {vs_endpoint.display_name} created with resource name {vs_endpoint.resource_name}\"\n",
        "    )\n",
        "    return vs_endpoint\n",
        "\n",
        "\n",
        "def deploy_index(\n",
        "    index: MatchingEngineIndex, endpoint: MatchingEngineIndexEndpoint\n",
        ") -> Optional[MatchingEngineIndexEndpoint]:\n",
        "    index_endpoints = [\n",
        "        (deployed_index.index_endpoint, deployed_index.deployed_index_id)\n",
        "        for deployed_index in index.deployed_indexes\n",
        "    ]\n",
        "\n",
        "    if len(index_endpoints) > 0:\n",
        "        vs_deployed_index = MatchingEngineIndexEndpoint(\n",
        "            index_endpoint_name=index_endpoints[0][0]\n",
        "        )\n",
        "        print(\n",
        "            f\"Vector Search index {index.display_name} is already deployed at endpoint {vs_deployed_index.display_name}\"\n",
        "        )\n",
        "        return vs_deployed_index\n",
        "\n",
        "    if not CREATE_RESOURCES:\n",
        "        print(\n",
        "            f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"\n",
        "        )\n",
        "        return None\n",
        "\n",
        "    print(\n",
        "        f\"Deploying Vector Search index {index.display_name} at endpoint {endpoint.display_name} ...\"\n",
        "    )\n",
        "    deployed_index_id = (\n",
        "        f'{VS_INDEX_NAME}_{create_uuid(VS_INDEX_NAME).split(\"-\")[-1]}'.replace(\"-\", \"_\")\n",
        "    )\n",
        "    vs_deployed_index = endpoint.deploy_index(\n",
        "        index=index,\n",
        "        deployed_index_id=deployed_index_id,\n",
        "        display_name=VS_INDEX_NAME,\n",
        "        machine_type=VS_MACHINE_TYPE,\n",
        "        min_replica_count=VS_MIN_REPLICAS,\n",
        "        max_replica_count=VS_MAX_REPLICAS,\n",
        "    )\n",
        "    print(\n",
        "        f\"Vector Search index {index.display_name} is deployed at endpoint {vs_deployed_index.display_name}\"\n",
        "    )\n",
        "    return vs_deployed_index\n",
        "\n",
        "\n",
        "def create_docai_processor(\n",
        "    processor_display_name: str = DOCAI_PROCESSOR_NAME,\n",
        "    processor_type: str = \"LAYOUT_PARSER_PROCESSOR\",\n",
        ") -> Optional[documentai.Processor]:\n",
        "    # Set the api_endpoint if you use a location other than 'us'\n",
        "    opts = ClientOptions(api_endpoint=f\"{DOCAI_LOCATION}-documentai.googleapis.com\")\n",
        "    docai_client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
        "    parent = docai_client.common_location_path(PROJECT_ID, DOCAI_LOCATION)\n",
        "    # Check if processor exists\n",
        "    processor_list = docai_client.list_processors(parent=parent)\n",
        "    processors = [\n",
        "        processor.name\n",
        "        for processor in processor_list\n",
        "        if (\n",
        "            processor.display_name == processor_display_name\n",
        "            and processor.type_ == processor_type\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    if len(processors) > 0:\n",
        "        docai_processor = docai_client.get_processor(name=processors[0])\n",
        "        print(\n",
        "            f\"Document AI processor {docai_processor.display_name} is already created\"\n",
        "        )\n",
        "        return docai_processor\n",
        "\n",
        "    if not CREATE_RESOURCES:\n",
        "        print(\n",
        "            f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"\n",
        "        )\n",
        "        return None\n",
        "\n",
        "    # Create a processor\n",
        "    print(\n",
        "        f\"Creating Document AI processor {processor_display_name} of type {processor_type} ...\"\n",
        "    )\n",
        "    docai_processor = docai_client.create_processor(\n",
        "        parent=parent,\n",
        "        processor=documentai.Processor(\n",
        "            display_name=processor_display_name, type_=processor_type\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Document AI processor {processor_display_name} of type {processor_type} is created.\"\n",
        "    )\n",
        "    return docai_processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3-wl01V_tfpS"
      },
      "outputs": [],
      "source": [
        "# @title Utility methods for adding index to Vertex AI Vector Search\n",
        "\n",
        "\n",
        "def get_batches(items: List, n: int = 1000) -> List[List]:\n",
        "    n = max(1, n)\n",
        "    return [items[i : i + n] for i in range(0, len(items), n)]\n",
        "\n",
        "\n",
        "def add_data(vector_store, chunks) -> None:\n",
        "    if RUN_INGESTION:\n",
        "        batch_size = 1000\n",
        "        texts = get_batches([chunk.page_content for chunk in chunks], n=batch_size)\n",
        "        metadatas = get_batches([chunk.metadata for chunk in chunks], n=batch_size)\n",
        "\n",
        "        for i, (b_texts, b_metadatas) in enumerate(zip(texts, metadatas)):\n",
        "            print(f\"Adding {len(b_texts)} data points to index\")\n",
        "            is_complete_overwrite = bool(i == 0)\n",
        "            vector_store.add_texts(\n",
        "                texts=b_texts,\n",
        "                metadatas=b_metadatas,\n",
        "                is_complete_overwrite=is_complete_overwrite,\n",
        "            )\n",
        "    else:\n",
        "        print(\"Skipping ingestion. Enable `RUN_INGESTION` flag\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS_MWLb6ESiS"
      },
      "outputs": [],
      "source": [
        "# @title Utility methods for displaying rich content results\n",
        "\n",
        "\n",
        "def get_chunk_content(results: List) -> List:\n",
        "    return [\n",
        "        doc.page_content.replace(\"\\n\", \"<br>\")\n",
        "        + f'<br><br> <b><a href=\"\">Source: {doc.metadata.get(\"source\")}</a></b>'\n",
        "        for doc in results\n",
        "    ][:5]\n",
        "\n",
        "\n",
        "CONTRASTING_COLORS = [\n",
        "    \"rgba(255, 0, 0, 0.2)\",  # Semi-transparent red\n",
        "    \"rgba(0, 255, 0, 0.2)\",  # Semi-transparent green\n",
        "    \"rgba(0, 0, 255, 0.2)\",  # Semi-transparent blue\n",
        "    \"rgba(255, 255, 0, 0.2)\",  # Semi-transparent yellow\n",
        "    \"rgba(0, 255, 255, 0.2)\",  # Semi-transparent cyan\n",
        "    \"rgba(255, 0, 255, 0.2)\",  # Semi-transparent magenta\n",
        "    \"rgba(255, 165, 0, 0.2)\",  # Semi-transparent orange\n",
        "    \"rgba(255, 105, 180, 0.2)\",  # Semi-transparent pink\n",
        "    \"rgba(75, 0, 130, 0.2)\",  # Semi-transparent indigo\n",
        "    \"rgba(255, 192, 203, 0.2)\",  # Semi-transparent light pink\n",
        "    \"rgba(64, 224, 208, 0.2)\",  # Semi-transparent turquoise\n",
        "    \"rgba(128, 0, 128, 0.2)\",  # Semi-transparent purple\n",
        "    \"rgba(210, 105, 30, 0.2)\",  # Semi-transparent chocolate\n",
        "    \"rgba(220, 20, 60, 0.2)\",  # Semi-transparent crimson\n",
        "    \"rgba(95, 158, 160, 0.2)\",  # Semi-transparent cadet blue\n",
        "    \"rgba(255, 99, 71, 0.2)\",  # Semi-transparent tomato\n",
        "    \"rgba(144, 238, 144, 0.2)\",  # Semi-transparent light green\n",
        "    \"rgba(70, 130, 180, 0.2)\",  # Semi-transparent steel blue\n",
        "]\n",
        "\n",
        "\n",
        "def convert_markdown_to_html(text: str) -> str:\n",
        "    # Convert Markdown to HTML, ensuring embedded HTML is preserved and interpreted correctly.\n",
        "    md_extensions = [\n",
        "        \"extra\",\n",
        "        \"abbr\",\n",
        "        \"attr_list\",\n",
        "        \"def_list\",\n",
        "        \"fenced_code\",\n",
        "        \"footnotes\",\n",
        "        \"md_in_html\",\n",
        "        \"tables\",\n",
        "        \"admonition\",\n",
        "        \"codehilite\",\n",
        "        \"legacy_attrs\",\n",
        "        \"legacy_em\",\n",
        "        \"meta\",\n",
        "        \"nl2br\",\n",
        "        \"sane_lists\",\n",
        "        \"smarty\",\n",
        "        \"toc\",\n",
        "        \"wikilinks\",\n",
        "    ]\n",
        "    return str(md.markdown(text, extensions=md_extensions))\n",
        "\n",
        "\n",
        "# Utility function to create HTML table with colored results\n",
        "def display_html_table(simple_results: List[str], reranked_results: List[str]) -> None:\n",
        "    # Find all unique values in both lists\n",
        "    unique_values = set(simple_results + reranked_results)\n",
        "\n",
        "    # Ensure we have enough colors for all unique values\n",
        "    # If not, colors will repeat, which might not be ideal but is necessary if the number of unique values exceeds the number of colors\n",
        "    colors = CONTRASTING_COLORS * (len(unique_values) // len(CONTRASTING_COLORS) + 1)\n",
        "\n",
        "    # Create a dictionary to map each unique value to a color\n",
        "    color_map = dict(zip(unique_values, colors))\n",
        "\n",
        "    # Initialize the HTML table with style for equal column widths\n",
        "    html = \"\"\"\n",
        "    <style>\n",
        "    td, th {\n",
        "        padding: 8px;\n",
        "        text-align: left;\n",
        "        border-bottom: 1px solid #ddd;\n",
        "        color: #000;\n",
        "    }\n",
        "    tr {background-color: #ffffff;}\n",
        "    /* Set table layout to fixed to respect column widths */\n",
        "    table {\n",
        "        table-layout: fixed;\n",
        "        width: 100%; /* You can adjust the overall table width as needed */\n",
        "        max-height: 100vh !important; /* Set the maximum height of the table */\n",
        "        overflow-y: auto; /* Add a vertical scrollbar if the content exceeds the maximum height */\n",
        "    }\n",
        "    /* Set equal width for both columns */\n",
        "    td, th {\n",
        "        width: 50%;\n",
        "    }\n",
        "    .text-black {\n",
        "        color: #000; /* Set the text color to black */\n",
        "    }\n",
        "    </style>\n",
        "    <table>\n",
        "    <tr><th>Retriever Results</th><th>Reranked Results</th></tr>\n",
        "    \"\"\"\n",
        "    # Iterate over the results and assign the corresponding color to each cell\n",
        "    for simple, reranked in zip(simple_results, reranked_results):\n",
        "        html += f\"\"\"\n",
        "        <tr>\n",
        "            <td style='color: black; background-color: {color_map[simple]}; font-size: 8px;'>\n",
        "                <p class='text-black'>{convert_markdown_to_html(simple)}</p>\n",
        "            </td>\n",
        "            <td style='color: black; background-color: {color_map[reranked]}; font-size: 8px;'>\n",
        "                <p class='text-black'>{convert_markdown_to_html(reranked)}</p>\n",
        "            </td>\n",
        "        </tr>\n",
        "        \"\"\"\n",
        "    html += \"</table>\"\n",
        "    display(HTML(html))\n",
        "\n",
        "\n",
        "def get_sxs_comparison(\n",
        "    simple_retriever, reranking_api_retriever, query, search_kwargs\n",
        ") -> List:\n",
        "    simple_results = get_chunk_content(\n",
        "        simple_retriever.invoke(query, search_kwargs=search_kwargs)\n",
        "    )\n",
        "    reranked_results = get_chunk_content(\n",
        "        reranking_api_retriever.invoke(query, search_kwargs=search_kwargs)\n",
        "    )\n",
        "    display_html_table(simple_results, reranked_results)\n",
        "\n",
        "    return reranked_results\n",
        "\n",
        "\n",
        "def display_grounded_generation(response) -> None:\n",
        "    # Extract the answer with citations and cited chunks\n",
        "    answer_with_citations = response.answer_with_citations\n",
        "    cited_chunks = response.cited_chunks\n",
        "\n",
        "    # Build HTML for the chunks\n",
        "    chunks_html = \"\".join(\n",
        "        [\n",
        "            f\"<div id='chunk-{index}' class='chunk'>\"\n",
        "            + f\"<div class='source'>Source {index}: <a href='{chunk['source'].metadata['source']}' target='_blank'>{chunk['source'].metadata['source']}</a></div>\"\n",
        "            + f\"<p>{chunk['chunk_text']}</p>\"\n",
        "            + \"</div>\"\n",
        "            for index, chunk in enumerate(cited_chunks)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Replace citation indices with hoverable spans\n",
        "    for index in range(len(cited_chunks)):\n",
        "        answer_with_citations = answer_with_citations.replace(\n",
        "            f\"[{index}]\",\n",
        "            f\"<span class='citation' onmouseover='highlight({index})' onmouseout='unhighlight({index})'>[{index}]</span>\",\n",
        "        )\n",
        "\n",
        "    # The complete HTML\n",
        "    html_content = f\"\"\"\n",
        "    <style>\n",
        "    body {{\n",
        "        font-family: Arial, sans-serif;\n",
        "        background-color: #e7f0fd;\n",
        "        padding: 20px;\n",
        "    }}\n",
        "    .answer-box {{\n",
        "        background-color: #f8f9fa;\n",
        "        border-left: 4px solid #0056b3;\n",
        "        padding: 20px;\n",
        "        margin-bottom: 20px;\n",
        "        color: #000;\n",
        "    }}\n",
        "    .citation {{\n",
        "        background-color: transparent;\n",
        "        cursor: pointer;\n",
        "    }}\n",
        "    .chunk {{\n",
        "        background-color: #ffffff;\n",
        "        border-left: 4px solid #007bff;\n",
        "        padding: 10px;\n",
        "        margin-bottom: 10px;\n",
        "        transition: background-color 0.3s;\n",
        "        color: #000;\n",
        "    }}\n",
        "    .source {{\n",
        "        font-weight: bold;\n",
        "        margin-bottom: 5px;\n",
        "    }}\n",
        "    a {{\n",
        "        text-decoration: none;\n",
        "        color: #0056b3;\n",
        "    }}\n",
        "    a:hover {{\n",
        "        text-decoration: underline;\n",
        "    }}\n",
        "    </style>\n",
        "    <div class='answer-box'>{answer_with_citations}</div>\n",
        "    <div class='chunks-box'>{chunks_html}</div>\n",
        "    <script>\n",
        "    function highlight(index) {{\n",
        "        // Highlight the citation in the answer\n",
        "        document.querySelectorAll('.citation').forEach(function(citation) {{\n",
        "            if (citation.textContent === '[' + index + ']') {{\n",
        "                citation.style.backgroundColor = '#ffff99';\n",
        "            }}\n",
        "        }});\n",
        "        // Highlight the corresponding chunk\n",
        "        document.getElementById('chunk-' + index).style.backgroundColor = '#ffff99';\n",
        "    }}\n",
        "    function unhighlight(index) {{\n",
        "        // Unhighlight the citation in the answer\n",
        "        document.querySelectorAll('.citation').forEach(function(citation) {{\n",
        "            if (citation.textContent === '[' + index + ']') {{\n",
        "                citation.style.backgroundColor = 'transparent';\n",
        "            }}\n",
        "        }});\n",
        "        // Unhighlight the corresponding chunk\n",
        "        document.getElementById('chunk-' + index).style.backgroundColor = '#ffffff';\n",
        "    }}\n",
        "    </script>\n",
        "    \"\"\"\n",
        "    display(HTML(html_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZLW0DvVfil_"
      },
      "source": [
        "# ‚öôÔ∏è Initialize resources\n",
        "\n",
        "The DIY RAG application requires the following resources, which will be provisioned by this step if not already present:\n",
        "\n",
        "- Document AI Layout Parser processor to parse the input documents\n",
        "- Vertex AI Vector Search index and endpoint to host the index for vector search\n",
        "- Cloud Storage bucket to store documents\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>‚ö†Ô∏è Resource creation will be skipped if <code>CREATE_RESOURCES</code> flag is set to <code>False</code> in the Initialize Variables section.  ‚ö†Ô∏è</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B0TFTm_fjES"
      },
      "outputs": [],
      "source": [
        "if CREATE_RESOURCES:\n",
        "    print(\"Creating new resources.\")\n",
        "else:\n",
        "    print(\"Resource creation is skipped.\")\n",
        "\n",
        "# Create bucket if not exists\n",
        "bucket = create_bucket(GCS_BUCKET_NAME)\n",
        "\n",
        "# Create vector search index if not exists else return index resource name\n",
        "vs_index = create_index()\n",
        "\n",
        "# Create vector search index endpoint if not exists else return index endpoint resource name\n",
        "vs_endpoint = create_index_endpoint()\n",
        "\n",
        "# Deploy index to the index endpoint\n",
        "deploy_index(vs_index, vs_endpoint)\n",
        "\n",
        "# Create Document Layout Processor\n",
        "docai_processor = create_docai_processor(processor_display_name=DOCAI_PROCESSOR_NAME)\n",
        "PROCESSOR_NAME = docai_processor.name  # DocAI Layout Parser Processor Name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpqeS0bcWJHs"
      },
      "source": [
        "# üì• Data Ingestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQE-Pll57PvM"
      },
      "source": [
        "## üìÑ Document Processing and Indexing\n",
        "\n",
        "This steps reads documents from Cloud Storage bucket, parses them using Document AI layout processor, extracts chunks from the parsed document, generates emebeddings using Vertex AI Embeddings API and add them to the Vertex AI Vector Search index.\n",
        "\n",
        "[These](https://cloud.google.com/generative-ai-app-builder/docs/prepare-data#storage-unstructured) are some sample public datasets available in GCS for usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po9jMRoIpNrc"
      },
      "source": [
        "### Step 1. Process Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e708g8Uno3P"
      },
      "source": [
        "**1.1 Read document paths from Cloud Storage bucket**\n",
        "\n",
        "Here we are reading documents from a public Cloud Storage bucket with Alphabet investor reports for years 2021, 2022 and 2023. You can replace them with your own documents hosted in Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUyTDEjZ9tFE"
      },
      "outputs": [],
      "source": [
        "loader = GCSDirectoryLoader(\n",
        "    project_name=PROJECT_ID,\n",
        "    bucket=\"github-repo\",\n",
        "    prefix=\"search/vais-standalone-apis/alphabet-investor-pdfs\",\n",
        ")\n",
        "doc_blobs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUw84zmOSU4h"
      },
      "source": [
        "**1.2 Parse raw documents and chunk them**\n",
        "\n",
        "We will be utilizing the Document AI Layout Parser to read files from Cloud Storage bucket as Blobs and then convert them as **layout-aware** chunks. Layout Parser extracts document content elements like text, tables, and lists, and creates context-aware chunks that are incredibly useful for building RAG applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNUi18eSpZGH"
      },
      "source": [
        "- Define Document AI Layout Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9YTgEv7FxhQ"
      },
      "outputs": [],
      "source": [
        "parser = DocAIParser(\n",
        "    project_id=PROJECT_ID,\n",
        "    location=DOCAI_LOCATION,\n",
        "    processor_name=PROCESSOR_NAME,\n",
        "    gcs_output_path=GCS_OUTPUT_PATH,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XMYwCKopapw"
      },
      "source": [
        "- Process the documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Loo3qfqbE9rj"
      },
      "outputs": [],
      "source": [
        "docs = list(\n",
        "    parser.batch_parse(\n",
        "        doc_blobs,  # filter only last 40 for docs after 2020\n",
        "        chunk_size=500,\n",
        "        include_ancestor_headings=True,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApYlM1C-ayJj"
      },
      "source": [
        "- Examine a chunk\n",
        "\n",
        "Let's examine one of the chunks. Notice that the document is parsed into different sections like title, subtitle and even a markdown table (especially a complex table with merged cells!).\n",
        "\n",
        "This makes it easy for retrieval as well for the downstream generation tasks. For example, LLM can now reason more effectively and more accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MF44xFkhhqL"
      },
      "outputs": [],
      "source": [
        "print(docs[1].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSmxytkUgDQL"
      },
      "source": [
        "### Step 2: Index the chunk embeddings\n",
        "\n",
        "The previous chunks of text are still just text. This step creates [embeddings](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) of the text chunks returned from the layout parser and upserts them into [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview) index. \n",
        "\n",
        "Next up we will then use Vertex AI Vector Search as a retriever for the RAG pipeline. Vector Search offers blazing fast retrieval that scales to billions of vectors with high recall, resulting in better searches at speed.\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>‚ö†Ô∏è Remember to run the Initialize Resources section to create and configure Vector Search index. ‚ö†Ô∏è</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHIVLse6trHc"
      },
      "source": [
        "**2.1 Define the model for creating embeddings.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B10-Kw03tshI"
      },
      "outputs": [],
      "source": [
        "embedding_model = VertexAIEmbeddings(model_name=EMBEDDINGS_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNZAHGIBuASE"
      },
      "source": [
        "**2.2 Initialize the Vertex AI Vector Search retriever.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATAUOB1ZuI3S"
      },
      "outputs": [],
      "source": [
        "vector_store = VectorSearchVectorStore.from_components(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    gcs_bucket_name=GCS_BUCKET_NAME,\n",
        "    index_id=vs_index.resource_name,\n",
        "    endpoint_id=vs_endpoint.resource_name,\n",
        "    embedding=embedding_model,\n",
        "    stream_update=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzDLmrfPueQE"
      },
      "source": [
        "**2.3 Store chunks as embeddings in the Vector Search index and raw texts in the Cloud Storage bucket.**\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>‚ö†Ô∏è To skip ingestion and query pre-indexed documents, set  <code>RUN_INGESTION</code> <code>False</code>.\n",
        "‚ö†Ô∏è</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHlnfPeWrkCK"
      },
      "outputs": [],
      "source": [
        "add_data(vector_store, docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpfCv4N-WTkh"
      },
      "source": [
        "# ü§ñ Serving\n",
        "\n",
        "All of the setup is done.  You retrieved source documents, processed and chunked them, embedded them into vectors and upserted them into Vector Search.  \n",
        "\n",
        "Now it's time to do some searches and generate grounded text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aeTofJ_viig"
      },
      "source": [
        "## üîé Retrieval and Ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi6i4Im6ikMh"
      },
      "source": [
        "### Step 3. Retrieve and Re-rank Chunks\n",
        "\n",
        "In this step, Vertex AI Vector Search retrieves the top-k relevant results, which are then re-ranked by Vertex AI Ranking API based on chunk content and semantic similarity to the query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1edec0fc55b8"
      },
      "source": [
        "More on the Vertex Search Ranking API:\n",
        "\n",
        "> The Vertex AI Search Ranking API is one of the standalone APIs in Vertex AI Agent Builder. It takes a list of documents and reranks those documents based on how relevant the documents are to a query. Compared to embeddings, which look only at the semantic similarity of a document and a query, the ranking API can give you precise scores for how well a document answers a given query. The ranking API can be used to improve the quality of search results after retrieving an initial set of candidate documents.\n",
        ">\n",
        "> The ranking API is stateless so there's no need to index documents before calling the API. All you need to do is pass in the query and documents. This makes the API well suited for reranking documents from any document retrievers.\n",
        ">\n",
        "> For more information, see [Rank and rerank documents](https://cloud.google.com/generative-ai-app-builder/docs/ranking)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOWVp-wSwrxQ"
      },
      "source": [
        "**3.1 Define and combine retriever using Vector Search and reranker using the Vertex AI Ranking API.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtG2ohe5vvWc"
      },
      "outputs": [],
      "source": [
        "# Instantiate the VertexAIReranker with the SDK manager\n",
        "reranker = VertexAIRank(\n",
        "    project_id=PROJECT_ID,\n",
        "    location_id=\"global\",\n",
        "    ranking_config=\"default_ranking_config\",\n",
        "    title_field=\"source\",  # metadata field to preserve with reranked results\n",
        "    top_n=5,\n",
        ")\n",
        "\n",
        "basic_retriever = vector_store.as_retriever(\n",
        "    search_kwargs={\"k\": 5}\n",
        ")  # fetch top 5 documents\n",
        "\n",
        "# Create the ContextualCompressionRetriever with the VertexAIRanker as a Reranker\n",
        "retriever_with_reranker = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker, base_retriever=basic_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljM5sDMhmFte"
      },
      "source": [
        "**3.2 Examine results before and after re-ranking**\n",
        "\n",
        "See the difference reranking makes! By prioritizing semantically relevant documents, the Ranking API improves the LLM's context, leading to more accurate and well-reasoned answers. Compare the `Retriever Results` and the `Reranked Results` side-by-side to see the improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdwMXWBqERVD"
      },
      "outputs": [],
      "source": [
        "reranked_results = get_sxs_comparison(\n",
        "    simple_retriever=basic_retriever,\n",
        "    reranking_api_retriever=retriever_with_reranker,\n",
        "    query=\"what was google cloud revenue in 2023 ?\",\n",
        "    search_kwargs={\"k\": 5},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clR7lhMExu4k"
      },
      "source": [
        "## üí¨ Answer Generation\n",
        "\n",
        "You have retrieved the most relevant facts from the all of your indexed source data.  Now we pass those facts into the LLM for answer generation, which will be grounded on the facts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uSrJukEm0ff"
      },
      "source": [
        "### Step 4. Query in Real Time and Check Grounding\n",
        "\n",
        "Let's now configure a standard retrieval and answer generation chain that follows: `query` -> `vector search` -> `retrieve documents` -> `LLM for answer generation` with a couple of changes:\n",
        "\n",
        "1. We will pass retrieved documents to the reranker API via the `VertexAIRank` and get the reranked documents to generate the answer.\n",
        "\n",
        "2. After the answer is generated by the LLM, pass the answer and the retrieved documents from vector search as facts to the `VertexAICheckGroundingWrapper` to check how grounded the response from the LLM is.\n",
        "\n",
        "More on the Vertex AI Check Grounding API:\n",
        "\n",
        "> The [Vertex AI Check Grounding API](https://cloud.google.com/generative-ai-app-builder/docs/check-grounding) is one of the standalone APIs in [Vertex AI Agent Builder](https://cloud.google.com/generative-ai-app-builder/docs/builder-apis). It is used to determine how grounded a piece of text (called an answer candidate) is in a given set of reference texts (called facts).\n",
        "\n",
        "> The Check Grounding API returns an overall support score of 0 to 1, which indicates how much the answer candidate agrees with the given facts. The response also includes citations to the facts supporting each claim in the answer candidate.\n",
        "\n",
        "> You can use the Check Grounding API for checking any piece of text. It could be a human-generated blurb or a machine-generated response. A typical use case would be to check an LLM-generated response with respect to a given set of facts. Among other things, the citations generated by the API would help distinguish hallucinated claim in the response from grounded claims.\n",
        "\n",
        "> For more information, see [Check Grounding](https://cloud.google.com/generative-ai-app-builder/docs/check-grounding)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg2dTwlD0DiK"
      },
      "source": [
        "**4.1 Define and configure retrieval and answer generation chain**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EY8ByQyzme8"
      },
      "source": [
        "- Configure retriever from the vector store previously defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igSgYSbizjjX"
      },
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma7Ia-pazrmZ"
      },
      "source": [
        "- Configure LLM with prompt template to generate answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZkK2hQnviqC"
      },
      "outputs": [],
      "source": [
        "llm = VertexAI(model_name=\"gemini-1.5-pro-001\", max_output_tokens=1024)\n",
        "template = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "create_answer = prompt | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oceQgfGgz60K"
      },
      "source": [
        "- Define wrapper to call Vertex AI Check Grounding API on the generated answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSziqSklz4WG"
      },
      "outputs": [],
      "source": [
        "output_parser = VertexAICheckGroundingWrapper(\n",
        "    project_id=PROJECT_ID,\n",
        "    location_id=\"global\",\n",
        "    grounding_config=\"default_grounding_config\",\n",
        "    top_n=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsBc1h150KZ8"
      },
      "source": [
        "- Define QA chain with Check Grounding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACDScPrbz6Qm"
      },
      "outputs": [],
      "source": [
        "@chain\n",
        "def check_grounding_output_parser(answer_candidate: str, documents: List[Document]):\n",
        "    return output_parser.with_config(configurable={\"documents\": documents}).invoke(\n",
        "        answer_candidate\n",
        "    )\n",
        "\n",
        "\n",
        "setup_and_retrieval = RunnableParallel(\n",
        "    {\"context\": retriever, \"query\": RunnablePassthrough()}\n",
        ")\n",
        "\n",
        "\n",
        "@chain\n",
        "def qa_with_check_grounding(query):\n",
        "    docs = setup_and_retrieval.invoke(query)\n",
        "    answer_candidate = create_answer.invoke(docs)\n",
        "    check_grounding_output = check_grounding_output_parser.invoke(\n",
        "        answer_candidate, documents=docs[\"context\"]\n",
        "    )\n",
        "    return check_grounding_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR7HCXOu0VU2"
      },
      "source": [
        "**4.2 Invoke Generation Generation API Chain.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ndeTmQiG0-V"
      },
      "outputs": [],
      "source": [
        "result = qa_with_check_grounding.invoke(\"what was google cloud revenue in 2023 ?\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFZ-ITDQ20B1"
      },
      "source": [
        "**4.3 Check grounding**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPF36wNTIVCe"
      },
      "outputs": [],
      "source": [
        "display_grounded_generation(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1_vSWFFwQZ3"
      },
      "outputs": [],
      "source": [
        "result = qa_with_check_grounding.invoke(\n",
        "    \"what were the downstream effects of covid on alphabet?\"\n",
        ")\n",
        "display_grounded_generation(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "786ea97a62ea"
      },
      "source": [
        "Congratulations!  You created a search engine from source documents, and wired in a real time RAG pipeline to retrieve only the most relevant facts and include them in your LLM generated responses, and you included a grounding verification step to ensure high quality results.\n",
        "\n",
        "If you would like to evaluate your generated answered on more dimensions, take a look at the [Vertex AI Eval Service metrics for RAG](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-examples#rag-qa) and you can get scores and explanations on many metrics like `question_answering_quality`, `question_answering_relevance`, `question_answering_helpfulness`, `groundedness`, `fulfillment`, `coherence`, `toxicity`, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDBtxT9B_BAI"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD5TKAQ53EoE"
      },
      "source": [
        "# üßπ Cleaning up\n",
        "\n",
        "Clean up resources created in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5oWfZxS3_He"
      },
      "outputs": [],
      "source": [
        "DELETE_DOCAI_PROCESSOR = False\n",
        "DELETE_INDEX = False\n",
        "DELETE_BUCKET = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-OQv0pt3xsi"
      },
      "source": [
        "- **Delete datapoints from Vector Search index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiqZ5hfy3nj_"
      },
      "outputs": [],
      "source": [
        "# Delete datapoints from Vertex AI Vector Store\n",
        "\n",
        "\n",
        "def delete_from_vector_search(\n",
        "    vs_index: MatchingEngineIndex,\n",
        "    vs_endpoint: MatchingEngineIndexEndpoint,\n",
        "    delete: bool = False,\n",
        "):\n",
        "    neighbors = vs_endpoint.find_neighbors(\n",
        "        deployed_index_id=vs_index.deployed_indexes[0].deployed_index_id,\n",
        "        queries=[[0.0] * VS_DIMENSIONS],\n",
        "        num_neighbors=5000,\n",
        "        return_full_datapoint=False,\n",
        "    )\n",
        "\n",
        "    datapoint_ids = [neighbor.id for neighbor in neighbors[0]]\n",
        "\n",
        "    # Delete datapoints\n",
        "    if delete:\n",
        "        print(f\"Deleting {len(datapoint_ids)} datapoints\")\n",
        "        response = vs_index.remove_datapoints(datapoint_ids=datapoint_ids)\n",
        "        print(response)\n",
        "\n",
        "\n",
        "delete_from_vector_search(vs_index, vs_endpoint, delete=DELETE_INDEX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHFQhYZh4hyR"
      },
      "source": [
        "- üóëÔ∏è **Remove Vertex AI Vector Search Index and Endpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcMlybUB3k6a"
      },
      "outputs": [],
      "source": [
        "if DELETE_INDEX:\n",
        "    print(f\"Undeploying all indexes and deleting the index endpoint {vs_endpoint}\")\n",
        "    vs_endpoint.undeploy_all()\n",
        "    vs_endpoint.delete()\n",
        "    print(f\"Deleting the index {vs_index}\")\n",
        "    vs_index.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTLMo_-t4oAK"
      },
      "source": [
        "- üóëÔ∏è **Remove Document AI Processor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhzflwGS34ju"
      },
      "outputs": [],
      "source": [
        "if DELETE_DOCAI_PROCESSOR:\n",
        "    docai_client = documentai.DocumentProcessorServiceClient()\n",
        "    request = documentai.DeleteProcessorRequest(name=docai_processor.name)\n",
        "    operation = docai_client.delete_processor(request=request)\n",
        "    print(\"Waiting for delete processor operation to complete...\")\n",
        "    response = operation.result()\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVks0uSk37PI"
      },
      "source": [
        "- üóëÔ∏è **Remove Google Cloud Storage bucket**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy9Q6IUd35yB"
      },
      "outputs": [],
      "source": [
        "if DELETE_BUCKET:\n",
        "    ! gsutil -m rm -r $STAGING_BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xWSwtzx-_Rj"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_VwREY0Orpy_",
        "afAL7KHUQxVA"
      ],
      "name": "vertex_ai_search_standalone_apis.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
